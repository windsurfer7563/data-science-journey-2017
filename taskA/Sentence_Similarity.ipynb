{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-24 10:44:40,296 : INFO : Loading dictionaries from C:\\WinPython\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\pymorphy2_dicts\\data\n",
      "2017-10-24 10:44:40,342 : INFO : format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-10T08:36:48.417827Z",
     "start_time": "2017-09-10T08:36:47.478946Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import functools\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "import semantics as sem\n",
    "import make_sentence_model as sen_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifierCV, LogisticRegressionCV\n",
    "import json\n",
    "import gensim\n",
    "import pymorphy2\n",
    "import pickle\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=2 ** 19)\n",
    "def uniq_words(text):\n",
    "    return set(re.findall(\"\\w+\", text))\n",
    "\n",
    "def calculate_idfs(data):\n",
    "    counter_paragraph = Counter()\n",
    "    uniq_paragraphs = data['p_bags'].unique()\n",
    "    for paragraph in uniq_paragraphs:\n",
    "        set_words = uniq_words(paragraph)\n",
    "        counter_paragraph.update(set_words)\n",
    "        \n",
    "    num_docs = uniq_paragraphs.shape[0]\n",
    "    idfs = {}\n",
    "    for word in counter_paragraph:\n",
    "        idfs[word] = np.log(num_docs / counter_paragraph[word])\n",
    "    return idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stop-words-russian.txt', encoding= 'utf-8' ) as f:\n",
    "    stopwords = f.readlines() \n",
    "stoplist = [word.replace('\\n','') for word in stopwords]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-10T08:36:50.759232Z",
     "start_time": "2017-09-10T08:36:48.419827Z"
    }
   },
   "outputs": [],
   "source": [
    "dftrain, dftest = pd.read_csv(\"data/train_task1_latest.csv\"), pd.read_csv(\"data/test_task1_latest.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = train_test_split(dftrain,test_size=0.3)\n",
    "train = train1.copy()\n",
    "test = test1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dftrain.copy()\n",
    "test = dftest.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-24 09:51:31,367 : INFO : loading projection weights from data/ruwikiruscorpora_0_300_20.bin.gz\n",
      "2017-10-24 09:51:51,323 : INFO : loaded (392339, 300) matrix from data/ruwikiruscorpora_0_300_20.bin.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec model 'data/ruwikiruscorpora_0_300_20.bin.gz' loaded\n"
     ]
    }
   ],
   "source": [
    "w2v_model = sem.load_w2v_model(sem.WORD2VEC_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_p = sen_model.read_paragraphs(pd.concat([train,test]),'paragraph')\n",
    "#model_q = sen_model.read_paragraphs(pd.concat([train,test]),'question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119398, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 119398/119398 [05:51<00:00, 1071.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74286, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 74286/74286 [03:27<00:00, 358.10it/s]\n"
     ]
    }
   ],
   "source": [
    "bags_p, bags_q =  sen_model.read_paragraphs(train)\n",
    "bags_p_test, bags_q_test =  sen_model.read_paragraphs(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex, fname in ((bags_p,'bags_p_dumps.json') , (bags_q,'bags_q_dumps.json'),(bags_p_test,'bags_p_test_dumps.json') , (bags_q_test,'bags_q_test_dumps.json')):\n",
    "     with open(fname, 'w') as f:\n",
    "        f.write(json.dumps(ex) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bags_p = json.load(open('bags_p_dumps.json'))\n",
    "#bags_q = json.load(open('bags_q_dumps.json'))\n",
    "#bags_p_test = json.load(open('bags_p_test_dumps.json'))\n",
    "#bags_q_test = json.load(open('bags_q_test_dumps.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sims_bags(row, b_p, b_q):\n",
    "        p_id = row['paragraph_id']\n",
    "        q_id = row['question_id']\n",
    "        sims = []\n",
    "        q_bag = b_q[str(q_id)]['bags'][0]\n",
    "        \n",
    "        sims_intersect = [sem.semantic_similarity(p_bag, q_bag, w2v_model) for p_bag in b_p[str(p_id)]['bags']]\n",
    "        sims = [s[0] for s in sims_intersect]\n",
    "        #all_intersections = [ins for s in sims_intersect for ins in s[1] if len(s[1]) > 0]\n",
    "        all_p_bags = [b for bags in b_p[str(p_id)]['bags'] for b in bags]\n",
    "        \n",
    "        return np.max(sims), np.mean(sims), ' '.join(all_p_bags), ' '. join(q_bag), ' '.join(list(set(all_intersections)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: train....\n",
      "Processing: test....\n",
      "Wall time: 12min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for name, df, b_p, b_q in [('train', train, bags_p, bags_q), ('test', test, bags_p_test, bags_q_test)]:\n",
    "        print(f'Processing: {name}....')\n",
    "        results = df.apply(lambda row: get_sims_bags(row, b_p,b_q), axis = 1)\n",
    "        df.loc[:,'max_sim'] = [r[0] for r in results]\n",
    "        df.loc[:,'avg_sim'] = [r[1] for r in results]\n",
    "        df.loc[:,'p_bags'] = [r[2] for r in results]\n",
    "        df.loc[:,'q_bag'] = [r[3] for r in results]\n",
    "        df.loc[:,'intersection'] = [r[3] for r in results]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 10:44:41,623 : INFO : Loading dictionaries from C:\\WinPython\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\pymorphy2_dicts\\data\n",
      "2017-10-25 10:44:41,684 : INFO : format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build features for train:  25%|████████▎                        | 30106/119398 [12:03<35:25, 42.01it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-190-cb334931b9bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'avg_sim'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m#df.loc[index, 'variance_sim'] = np.var(sims)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'p_bags'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_p_bags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'q_bag'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_bag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'intersection'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_intersections\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_has_valid_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    586\u001b[0m                 \u001b[1;31m# scalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m                     \u001b[0msetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36msetter\u001b[1;34m(item, v)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[1;31m# reset the sliced object if unique\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mcan_do_equal_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2329\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2330\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2331\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2396\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2397\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2398\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   2545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2546\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2547\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreindexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2549\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mreindexer\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m   2527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2528\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2529\u001b[1;33m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2530\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build features for train:  25%|████████▎                        | 30106/119398 [12:20<36:35, 40.68it/s]"
     ]
    }
   ],
   "source": [
    "'''# Old ver code above\n",
    "%time\n",
    "for name, df, b_p, b_q in [('train', train, bags_p, bags_q), ('test', test, bags_p_test, bags_q_test)]:\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"build features for \" + name):\n",
    "        p_id = row['paragraph_id']\n",
    "        q_id = row['question_id']\n",
    "        sims = []\n",
    "        q_bag = b_q[str(q_id)]['bags'][0]\n",
    "        \n",
    "        #all_p_bags = []\n",
    "        #all_intersections = []\n",
    "        #for p_bag in bags_p[p_id]['bags']:\n",
    "        #    sim, intersection = sem.semantic_similarity(p_bag, q_bag, w2v_model)\n",
    "        #    sims.append(sim)\n",
    "        #    if len(intersection) > 0: all_intersections.extend(intersection)\n",
    "        #    all_p_bags.extend(p_bag)\n",
    "        \n",
    "        sims_intersect = [sem.semantic_similarity(p_bag, q_bag, w2v_model) for p_bag in b_p[str(p_id)]['bags']]\n",
    "        sims = [s[0] for s in sims_intersect]\n",
    "        all_intersections = [ins for s in sims_intersect for ins in s[1] if len(s[1]) > 0]\n",
    "        all_p_bags = [b for bags in b_p[str(p_id)]['bags'] for b in bags]\n",
    "        \n",
    "        df.loc[index, 'max_sim'] = np.max(sims)\n",
    "        df.loc[index, 'avg_sim'] = np.mean(sims)\n",
    "        #df.loc[index, 'variance_sim'] = np.var(sims)\n",
    "        df.loc[index, 'p_bags'] = ' '.join(all_p_bags)\n",
    "        df.loc[index, 'q_bag'] = ' '. join(q_bag)\n",
    "        df.loc[index, 'intersection'] = ' '.join(list(set(all_intersections)))\n",
    "'''\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/train_t.csv', encoding = 'utf-8')\n",
    "test.to_csv('data/test_t.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build features for train: 100%|███████████████████████████████| 119398/119398 [10:04<00:00, 197.66it/s]\n",
      "build features for test: 100%|██████████████████████████████████| 74286/74286 [05:26<00:00, 227.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "idfs = calculate_idfs(train)\n",
    "for name, df in [('train', train), ('test', test)]:\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"build features for \" + name):\n",
    "        #question = uniq_words(row.q_bags)\n",
    "        #paragraph = uniq_words(row.p_bag)\n",
    "        question = set(row.q_bag.split())\n",
    "        paragraph = set(row.p_bags.split())\n",
    "        df.loc[index, 'len_paragraph'] = len(paragraph)\n",
    "        df.loc[index, 'len_question'] = len(question)\n",
    "        df.loc[index, 'len_intersection'] = len(paragraph & question)\n",
    "        df.loc[index, 'idf_question'] = np.sum([idfs.get(word, 0.0) for word in question])\n",
    "        df.loc[index, 'idf_paragraph'] = np.sum([idfs.get(word, 0.0) for word in paragraph])\n",
    "        df.loc[index, 'idf_intersection'] = np.sum([idfs.get(word, 0.0) for word in paragraph & question])\n",
    "        df.loc[index, 'idf_intersection2'] = np.sum([idfs.get(word, 0.0) for word in df.loc[index, 'intersection']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-330-55cd5c267dc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'max_max_sims'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_max_max_sims_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'max_is_not_max'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_max_sims\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_sim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_max_max_sims_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "#to find question that better fir to another paragraph\n",
    "def find_max_max_sims_train(row):\n",
    "    return train[train.question_id == row.question_id].max_sim.max()\n",
    "\n",
    "train.loc[:,'max_max_sims'] = train.apply(find_max_max_sims_train, axis = 1)\n",
    "train.loc[:,'max_is_not_max'] = train.max_max_sims != text.max_sim\n",
    "\n",
    "def find_max_max_sims_test(row):\n",
    "    return test[test.question_id == row.question_id].max_sim.max()\n",
    "\n",
    "test.loc[:,'max_max_sims'] = test.apply(find_max_max_sims_test, axis = 1)\n",
    "test.loc[:,'max_is_not_max'] = test.max_max_sims != text.max_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score = 0.9690617902195764, at tree number = 372\n",
      "Best Score2= 0.9073257466110525, at tree number = 297\n"
     ]
    }
   ],
   "source": [
    "columns1 = ['len_paragraph', 'len_question', 'len_intersection', 'idf_question', 'idf_paragraph', 'idf_intersection']\n",
    "columns2 = ['len_paragraph', 'len_question', 'max_sim', 'avg_sim', 'max_is_not_max']\n",
    "\n",
    "X_train, X_test = train_test_split(train, test_size=0.3)\n",
    "clf1 = GradientBoostingClassifier(n_estimators = 400)\n",
    "model1 = clf1.fit(X_train[columns1], X_train['target'])\n",
    "\n",
    "scores1 = [roc_auc_score(X_test['target'], predict) for predict in clf1.staged_predict(X_test[columns1])]\n",
    "best_tree_num1 = np.argmax(scores1) + 1\n",
    "max_score1 = np.max(scores1)\n",
    "print(\"Best Score = {}, at tree number = {}\".format(max_score1, best_tree_num1))\n",
    "\n",
    "\n",
    "clf2 = GradientBoostingClassifier(n_estimators = 300)\n",
    "model2 = clf2.fit(X_train[columns2], X_train['target'])\n",
    "scores2 = [roc_auc_score(X_test['target'], predict) for predict in clf2.staged_predict(X_test[columns2])]\n",
    "best_tree_num2 = np.argmax(scores2) + 1\n",
    "max_score2 = np.max(scores2)\n",
    "print(\"Best Score2= {}, at tree number = {}\".format(max_score2, best_tree_num2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score = 0.7381004447091776\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "clf2 = LogisticRegressionCV(cv =3)\n",
    "model2 = clf2.fit(X_train[columns2], X_train['target'])\n",
    "max_score2  = roc_auc_score(X_test['target'], model2.predict(X_test[columns2]))\n",
    "\n",
    "print(\"Best Score = {}\".format(max_score2))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score = 0.9891113738234695\n"
     ]
    }
   ],
   "source": [
    "clf1 = GradientBoostingClassifier(n_estimators = best_tree_num1)\n",
    "model1 = clf1.fit(X_train[columns1], X_train['target'])\n",
    "\n",
    "clf1 = GradientBoostingClassifier(n_estimators = best_tree_num2)\n",
    "model2 = clf1.fit(X_train[columns2], X_train['target'])\n",
    "\n",
    "predict1 = model1.predict(X_test[columns1])\n",
    "predict2 = model2.predict(X_test[columns2])\n",
    "\n",
    "predict = 0.9*predict1 + 0.1*predict2\n",
    "scores = roc_auc_score(X_test['target'], predict)\n",
    "\n",
    "print(\"Score = {}\".format(scores))       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = GradientBoostingClassifier(n_estimators = best_tree_num1)\n",
    "model1 = clf1.fit(train[columns1], train['target'])\n",
    "\n",
    "clf2 = GradientBoostingClassifier(n_estimators = best_tree_num2)\n",
    "model2 = clf2.fit(train[columns2], train['target'])\n",
    "\n",
    "predict1 = model1.predict_proba(test[columns1])[:, 1]\n",
    "predict2 = model2.predict_proba(test[columns2])[:, 1]\n",
    "\n",
    "predict = 0.9*predict1 + 0.1*predict2\n",
    "\n",
    "test['prediction'] = predict\n",
    "\n",
    "#clf = GradientBoostingClassifier(n_estimators = best_tree_num)\n",
    "#model = clf.fit(train[columns], train['target'])\n",
    "#test['prediction'] = model.predict_proba(test[columns])[:, 1]\n",
    "#test['prediction_1'] = model.predict(test[columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['paragraph_id', 'question_id', 'prediction']].to_csv(\"prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range = (2,2))\n",
    "quest_cv_test = cv.fit_transform(test['question'])\n",
    "test['max_words_count'] = np.amax(quest_cv_test, axis = 1).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test.max_words_count > 1, 'prediction'] = 0.05\n",
    "#test['len-quest'] = len(test.question)\n",
    "#test.loc[test.question.str.len() > 220, 'prediction'] = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#to find question that better fir to another paragraph\n",
    "def find_max_max_sims(row):\n",
    "    return test[test.question_id == row.question_id].max_sim.max()\n",
    "\n",
    "test.loc[:,'max_max_sims'] = test.apply(find_max_max_sims, axis = 1)\n",
    "test.loc[test.max_sim != test.max_max_sims,'prediction'] = 0 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['paragraph_id', 'question_id', 'prediction']].to_csv(\"prediction.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
